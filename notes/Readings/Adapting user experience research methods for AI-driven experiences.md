# Adapting user experience research methods for AI-driven experiences
See [[Adapting user experience research methods for AI-driven experiences.pdf]]

Proposes a framework for conducting UX research for AI-driven experiences

## AI-Driven Experiences: What's Unique?
- (It has some autonomy) 
  - It is co-steered by the AI, e.g. it auto-corrects some text
  - harder to predict how the user will respond
  - Some find it useful, some find it creepy
- Needs a new method of prototyping
  - The model can change
  - It requires more data to function
- The system is only partially explainable --> user might not trust the system
  - If the user understands how the system makes decisions, they're more likely to trust it

## Differences Require A New Framework For Selecting Methods
The probabilistic results of AI models make the standard frameworks harder (or impossible) to use and require some adaptations

It should also consider ethical problems, e.g. a system to assist elderly people with a built-in e-commerce platform that nudges the user to spend more. Is this what we should create?

## The Framework
![[The Framework]]

## Discussion
TODO:
Thus, classic techniques to specify an
envisioned experience such as “customer journeys,” 
“user stories,” “scenarios,” or “jobs to be done” now 
need to also incorporate information about probabilities 
of that experience

TODO:
Second, researchers will often face the problem of 
testing design concepts that have no market analogs. 
This means that the study participants will not have 
prior experiences as a frame of reference to assess the 
utility of and comfort with the new technology. Thus, 
prototyping and study set up need to compensate for 
this lack of prior experience; the features of the new 
product must be easy to understand for a novice user

TODO:
Third, adoption is a decision that synthesizes multiple 
factors such as perceived utility and ease of use. This is 
true for any technology, but AI-based systems 
introduce new factors that affect trust and therefore 
adoption—primarily “creepiness” and emotional factors. 
For example, an AI-driven personal memory feature in 
a social network application may resurface or create 
new content highlighting deceased family members and 
cause negative emotions that break trust in the system
